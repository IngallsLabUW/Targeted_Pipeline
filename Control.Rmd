---
title: "Targeted Pipeline"
author: "RLionheart"
date: "09/4/2022"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: inline
---

This markdown script controls the targeted pipeline for targeted data. 
The script handles data from both QE and TQS instruments, 
as well as data processed by Skyline and MSDIAL metabolomics processing platforms.

It contains four major sections: 
Section I: Import and cleaning/rearranging of data.
Section II: Quality control using user-defined parameters. 
Section III: Applying Best-Matched Internal Standard (B-MIS).
Section IV: Quantifying peak area to umol/vial when possible.

In order to run this script, you will need the following items:
1. The instrument output in csv format from either MSDIAL or Skyline.
2. The Ingalls Standards sheet, located on the Ingalls GitHub: https://github.com/IngallsLabUW/Ingalls_Standards

*This is a template for the Ingalls Targeted Pipeline! You will need to adjust it to reflect your data!*
*You will NEVER be able to just plug in data and have this work!!*

If you are running this pipeline for the first time, the first chunk (below) will populate your directory with a series of empty folders (data_raw, data_intermediate, etc) that will be used throughout the pipeline. You will need to fill the data_extras and data_raw folders with the appropriate files, either from the Ingalls Google Drive (see) if you are running the template, or with your own data for real analysis.

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(anytime)
library(data.table)
library(rlist)
library(tidyr)
library(tidyverse)
options(scipen=999)
currentDate <- Sys.Date()

source("src/Functions.R")

processed.folder <- "data_processed" 
intermediate.folder <- "data_intermediate"
extras.folder <- "data_extras"
raw.folder <- "data_raw"
figures.folder <- "figures"

dir.create(file.path(".", processed.folder))  
dir.create(file.path(".", intermediate.folder))  
dir.create(file.path(".", extras.folder))
dir.create(file.path(".", raw.folder))
dir.create(file.path(".", figures.folder))  

if (length(dir(path = "data_processed/")) == 0) {
  cat("\nData_processed subdirectory has been created and is ready for new data.")
} else {
  cat("Warning: some or all of the subdirectories are not empty. Empty contents before continuing.\n")
}
```


If you would like to empty the data_processed/, data_intermediate, and figures/ subdirectories, run the following code.
```{r, include = TRUE}
toClear <- c("data_processed/", "data_intermediate", "figures/")
f <- list.files(toClear, include.dirs = F, full.names = T, recursive = T)
file.remove(f)

print("Subdirectories emptied.")
```

----------------------------------------------------------------------------------------------------------------------------
Section I: 
For MSDIAL, import all MSDIAL files that have been split by Area, Mass/charge (Mz), Retention Time (RT), and Signal to Noise (SN). 
For Skyline, import your single Skyline file which should be in long format. Once files are imported and cleaned, ensure that the compound names are updated with the latest Ingalls Standards sheet.

Steps for MSDIAL data:
Set header, filter unknowns.
Change variable classes from character/factor to numeric, according to column.
Rearrange the data frames and combine to a single frame in long format.
Standardize dataset by removing "Ingalls_" prefixes from compound names, and removing the syntactically correct "X" from the Replicates column.

For Skyline data:
Ensure all columns are the appropriate class (numeric, character, etc.)

***
Inputs: 
"data_raw/*file.pattern*.csv
Outputs: 
"data_intermediates/*software.pattern*_combined_*file.pattern*_*DATE*.csv"
***

*User action required*
Comment or uncomment the file.pattern, software.pattern, instrument.pattern, and runtype.pattern required for your files.

The instrument.pattern refers to which instrument was used to analyze the data. Choose either "QE" or "TQS".
The software.pattern refers to MSDIAL or Skyline. Choose either "MSDIAL" or "Skyline".
The file.pattern is a user-defined variable that will help the program pick out the correct file from the data_raw folder. It should be unique to the file(s) you want to analyze.
The runtype.pattern refers to whether the data is HILIC (pos/neg) or Reverse Phase. Choose either "pos|neg" for HILIC or "reversephase" for reverse phase.


```{r Pattern matching, include = TRUE}
### Choose instrument pattern ###
## QE or TQS
# instrument.pattern <- "QE"
instrument.pattern <- "TQS"

### Choose software pattern ###
## MSDIAL or Skyline
# software.pattern <- "MSDIAL"
software.pattern <- "Skyline"

### Create file pattern ###
file.pattern <- "HRM_TQS_IonRatio"

### Choose matching pattern ###
# Reverse phase or HILIC
runtype.pattern <- "reversephase"
# runtype.pattern <- "pos|neg"

print(paste("Your software type is:", software.pattern))
print(paste("The instrument used in this run is:", instrument.pattern))
print(paste("Your file matching pattern is:", file.pattern))
print(paste("Your run type is:", runtype.pattern))
```


```{r Imports, include = TRUE}
source("src/File_Import.R")

print("Required files imported.")
```

*User action required*
This step changes depending on whether you are using Skyline or MSDIAL.

Enter the existing filenames of your run. The above chunk of code assigns the variables in R to their filename in the data_raw directory, so if your positive Area file is "data_processed/PositiveArea.csv", it has been imported to this environment as PositiveArea. Those files need to be reassigned to consistent names so the rest of the code will know which files to edit for the pipeline steps.

Comment or uncomment the block of variable names appropriate for your run.

```{r Dataset reassignment, include = TRUE}

### Skyline ###
## Reverse phase:
skyline.reversephase <- HRM_TQS_IonRatio_ReversePhase_Skyline_Example

## HILIC: 
# skyline.HILIC.pos <- HRM_TQS_HILICPos_Skyline_Example
# skyline.HILIC.neg <- HRM_TQS_HILICNeg_Skyline_Example

### MSDIAL ###
## HILIC:
# Area.positive <- MESOSCOPE_Area_HILICPos_MSDIAL_Example
# Mz.positive   <- MESOSCOPE_Mz_HILICPos_MSDIAL_Example
# RT.positive   <- MESOSCOPE_RT_HILICPos_MSDIAL_Example
# SN.positive   <- MESOSCOPE_SN_HILICPos_MSDIAL_Example
# 
# Area.negative <- MESOSCOPE_Area_HILICNeg_MSDIAL_Example
# Mz.negative   <- MESOSCOPE_Mz_HILICNeg_MSDIAL_Example
# RT.negative   <- MESOSCOPE_RT_HILICNeg_MSDIAL_Example
# SN.negative   <- MESOSCOPE_SN_HILICNeg_MSDIAL_Example

print(paste(file.pattern, instrument.pattern, software.pattern, "variables assigned."))
```

Check if dataset is MSDIAL or Skyline, rearrange if so, and export.
```{r Dataset rearrangement, include = TRUE}
if (software.pattern == "MSDIAL") {
  source("src/MSDIAL_Rearrange.R")
  print("Data rearrange of MSDIAL data complete. Exporting file to data_intermediate.")
} else {
  source("src/Skyline_Rearrange.R")
  print("Data rearrange of Skyline data complete. Exporting file to data_intermediate.")
}

# Clear environment
rm(list = setdiff(ls()[!ls() %in% c("file.pattern", "currentDate", "instrument.pattern", "software.pattern")], lsf.str()))
```

--------------------------------------------------------------

Section II: Quality Control and flagging of problematic peaks.

In the Quality Control Step for QE files:
Import files.
Identify run types and check if all are present (blk, smp, std, poo).
Create a table of standard retention times (RT) for comparison.
Create a table of areas from blank runs for comparison.
Flag peaks in the dataset that fall outside of user-defined parameters.
Add parameter values to the top of the final file and save to the data_processed/ folder.

Additional TQS step:
Create standard ion ratio table for comparison.

***
Inputs: 
"data_intermediate/*software.pattern*_combined_*file.pattern*_*DATE*.csv"

Outputs: 
"data_intermediate/*software.pattern*_*instrument.pattern*_RT.table_*DATE*.csv"
"data_intermediate/*software.pattern*_*instrument.pattern*_SN.table_*DATE*.csv"
"data_intermediate/*software.pattern*_*instrument.pattern*_area.table_*DATE*.csv"
"data_intermediate/*software.pattern*_*instrument.pattern*_final.table_*DATE*.csv"
"data_intermediate/*software.pattern*_*instrument.pattern*_blank.table_*DATE*.csv"
"data_processed/*instrument.pattern*_QC_Output_*file.pattern*_*DATE*.csv"
Additional TQS Output:
"data_processed/*instrument.pattern*_IR.table_*DATE*.csv"
***

*User action required*
Define parameters for quality control. These act as comparison for filtering out data.
The numbers will change depending on whether you are analyzing HILIC vs Reverse phase data, or if you are measuring TQS vs QE data.

```{r QC parameters, include = TRUE}
# QE + TQS QC parameters
area.min   <- 1000 # HILIC - 1000, Reverse phase - 5000
RT.flex    <- 0.4 # HILIC +/- 0.4 min, Reverse phase +/- 0.2 min 
blk.thresh <- 0.3 # HILIC +/- 0.3, Reverse phase +/- 0.2
SN.min     <- 4 # HILIC - 4, Reverse phase - 4

# Additional QC parameters for Skyline TQS 
height.min <- 1000
height.max <- 1.0e8

# Additional QC parameter for Ion Ratio TQS 
IR.flex  <- 0.3

print("Parameter values assigned.")
```

Run Quality Control and export.
```{r MSDIAL and Skyline QC, include=TRUE}
if (software.pattern == "MSDIAL") {
  source("src/MSDIAL_QC.R")
} else {
  source("src/Skyline_QC.R")  
}
```

Inspect the blank.table, final.table, and RT.table values, which currently exist in the environment.
Ensure that they look normal before proceeding to clear the environment in the next step.
```{r, include = TRUE}
currentDate <- Sys.Date()
csvFileName <- paste("data_processed/", software.pattern, "_", instrument.pattern,
                     "_QC_Output_", file.pattern, "_", currentDate, ".csv", sep = "")

tables <- grep("table", names(.GlobalEnv), value = TRUE, ignore.case = TRUE)
tablelist <- do.call("list", mget(tables))

# Write intermediate data
invisible(lapply(tables, 
                 function(x) write.csv(get(x), file=paste("data_intermediate/",
                                                            software.pattern, "_",
                                                          instrument.pattern,
                                                            "_", x, "_", currentDate,
                                                          ".csv", sep = ""))))
# Write final data
write.csv(final.table, csvFileName, row.names = FALSE)

print(paste(tables, "saved to data/intermediate"))

rm(list = setdiff(ls()[!ls() %in% c("file.pattern")], lsf.str()))
```

--------------------------------------------------------------

Section III: Best-Matched Internal Standard (B-MIS)

In the BMIS step:
If working with HILIC data, identify and remove duplicates. Justify your decision.
Match QC'd data with Internal Standards list.
Identify internal standards and visualize those areas.
Test if replicate names are identical across the data and the sample key. Stop the analysis process if not.
Identify/visualize internal standards. Look over it to ensure that things are looking more or less "normal".
Using pooled and sample runs, calculate internal standard averages.
Create an adjusted area for all pooled and sample runs by comparing each mass feature to each internal standard.
Calculate RSD for each mass feature/internal standard pair, and use this to choose a matched internal standard.
Decide whether to accept a B-MIS or not, according to user-defined values.

***
Inputs:
"data_processed/*software.pattern*_*instrument.pattern*_QC_Output_*file.pattern*_*DATE*.csv"
Most recent Ingalls Lab Standards from GitHub:
https://raw.githubusercontent.com/IngallsLabUW/Ingalls_Standards/master/Ingalls_Lab_Standards.csv


Outputs:
"figures/IS.Raw.Areas.png"
"figures/BMIS_Evalution.png"
"data_intermediate/*instrument.pattern*_InternalStdIssues_*DATE*.csv"
"data_intermediate/MSDIAL_QuickReport_*file.pattern*_*DATE*.txt"
"data_processed/MSDIAL_BMIS_Output_*file.pattern*_*DATE*.csv"
***

*User action required*
Enter user data for cut off parameters.
cut.off = Decrease in RSD of pooled injections, aka improvement cutoff.
cut.off2 = Relative squared deviation minimum.

```{r BMIS cutoff values, include = TRUE}
cut.off <- 0.4
cut.off2 <- 0.1

print("B-MIS Cutoff values assigned.")
```

*User action required*
Comment out appropriate variable blocks according to HILIC or Reverse phase data.
```{r Reverse phase HILIC assignment, include = TRUE}

# MSDIAL
software.pattern <- "MSDIAL"

# Skyline
#software.pattern <- "Skyline"

# Cyano Skyline
# Column.Type = "RP"
# standards.pattern = "Ingalls"
# QC.pattern = "QC_Output_CyanoAq"

# Cyano MSDIAL
# Column.Type = "RP"
# standards.pattern = "Ingalls"
# QC.pattern = "QC_Output_CYANO"

# HILIC Skyline
# Column.Type = "HILIC"
# standards.pattern = "Ingalls"
# QC.pattern = "QC_Output_HILIC"

# HILIC MSDIAL
Column.Type = "HILIC"
standards.pattern = "Ingalls"
QC.pattern = "QC_Output_HILIC"

print(paste(file.pattern, "column type, software pattern, standards pattern, and QC pattern assigned."))
```

Import required files.
```{r BMIS imports, include=TRUE}
source("src/BMIS_Imports.R")

print("BMIS files imported.")
```

If data is HILIC, identify duplicates and decide which to remove
IdentifyDuplicates function will confirm if instrument column data exists.
User will need to use best judgement to decide which duplicate to remove.
```{r Check duplicates, include=TRUE}
source("src/Check_Duplicates.R")
currentDate = Sys.Date()
csvFileName <- paste("data_intermediate/", software.pattern,
                     "_HILIC.duplicates_", currentDate, ".csv", sep = "")

# Using the duplicates.testing table, decide which detected compound to keep, the positive or negative.

if (exists("duplicates.testing") == TRUE) {
  print("This is a HILIC run. Look at duplicates.testing to decide which compounds to remove.")
  
  write.csv(HILICS.duplicates, csvFileName, row.names = FALSE)
} else {
  print("Non-HILIC data: no duplicates to remove.")
}

```

*User action required*
The below section automatically removes the HILICNeg compounds from the QC'd data.
This is not necessarily the correct solution: the user must look at the data to ensure that they are removing the right compounds.
```{r Remove duplicates, include = TRUE}
if (exists("duplicates.testing")) {
  QCd.data <- QCd.data %>%
    filter(!(Metabolite.Name %in% HILICS.duplicates$Metabolite.Name & Column == "HILICNeg")) # This line requires user attention.
  print("HILIC duplicates removed: negative ion mode.")

  Report <- print(paste(HILICS.duplicates$Metabolite.Name, "HILICNeg"))
  cat(Report, file = "data_intermediate/MSDIAL_HILIC_DuplicateReport.txt")

  write.csv(duplicates.testing, csvFileName, row.names = FALSE)
} else {
  print("Non-HILIC data: no duplicates removed.")
}

```

Run BMIS.
```{r BMIS, include=TRUE}
source("src/BMIS.R")
```

--------------------------------------------------------------

Section IV: Convert from peak area to umol/vial.

In the quantify step:
Get response factors and response factor ratios.
Quantify samples without an internal standard.
Quantify samples with an internal standard.
Accounting for dilution and filtered volume, calculate environmental quantities.
Summarize carbon and nitrogen.

***
Inputs:
"data_extras/Ingalls_Lab_Standards.csv"
"data_extras/*Internal standard concentrations relevant to your run*.csv"
"data_processed/BMIS_Output_*file.pattern*_*DATE*.csv"
"data_processed/MSDIAL_QC_Output_*file.pattern*_*DATE*.csv"

Outputs:
"data_intermediate/MSDIAL_ResponseFactorRatios_*DATE*.csv"
"data_processed/Quantified_Summary_*Column.Type*_*DATE*.csv
"data_processed/Quantified_Measurements*Column.Type*_*DATE*.csv
"data_processed/Quantified_perSampID_*Column.Type*_*DATE*.csv
***

*User action required*
Make sure you know your dilution factor, injection volume, any filtered volume from samples, etc., etc... Keep your units straight! Make sure you understand your units and keep your stoichiometry straight!
```{r, include = TRUE}
Dilution.Factor = 2
Reconstitution.Volume = 400 # microliters
Volume.Filtered = 5 # liters

print("Dilution factor, injection volume, and filtered volume assigned.")
```

*User action required*
Comment out appropriate variable blocks according to HILIC or Cyano data.
"BMIS.pattern" identifies the BMIS output from Section III.
"QC.pattern" identifies the QC'd output from Section II.
"names.pattern" identifies the Internal Standards names sheet (which will change depending on your run!).
"Column.Type" is reverse phase or HILIC.

```{r, include = TRUE}
# Cyano
# BMIS.pattern = "BMIS_Output_RP"
# QC.pattern = "QC_Output_CYANO"
# names.pattern = "Names"
# Column.Type = "RP"

# HILIC
BMIS.pattern = "BMIS_Output_HILIC"
QC.pattern = "QC_Output_Example"
names.pattern = "Names"
Column.Type = "HILIC"

print(paste(file.pattern, "matching patterns assigned."))

```

Import required files for quantification.
```{r Quantify imports, include = TRUE}
source("src/MSDIAL_Quantify_Imports.R")

print("Quantification files imported.")
```

Repeat the HILIC duplicates step.
```{r Check duplicates, include=TRUE}
check.duplicates <- list.files("data_intermediate", pattern = "duplicate", full.names = TRUE)

if (length(check.duplicates) == 0) {
  print("No HILIC data exists.")
} else {
  print("HILIC data exists, check for duplicates.")

  HILICS.duplicates <- read.csv(check.duplicates, stringsAsFactors = FALSE)
}
```

*User action required*
If you are working with HILIC data, make sure your filtering step makes sense.
```{r Check duplicates, include=TRUE}

if (length(check.duplicates) == 0) {
  print("No HILIC data exists.")
} else {
  HILICS.duplicates <- read.csv(check.duplicates, stringsAsFactors = FALSE)
  QCd.data <- QCd.data %>%
    filter(!(Metabolite.Name %in% HILICS.duplicates$Metabolite.Name & Column == "HILICNeg"))
  print("Duplicates removed.")
}

```

Check which kinds of standards have been run.
*****************************************************************************
This function is unlikely to work on all runs due to sampID differences.
It will almost definitely need to be modified to apply to the dataset at hand.
*****************************************************************************
```{r, include = TRUE}
Full.stds.data <- CheckStandards2(Full.stds.data)
```

Calculate the response factors in your data.
Carefully look at the output from this section! You will likely be missing response factors for compounds. 
```{r Calculate response factors, include=TRUE}
source("src/ResponseFactor_Calculation.R")

currentDate = Sys.Date()
write.csv(Full.data.RF.ratios, paste("data_intermediate/MSDIAL_ResponseFactorRatios_", currentDate, ".csv", sep = ""))

```


Quantify.
```{r, include = TRUE}
source("src/MSDIAL_Quantify.R")
```

Review the "Final" files to ensure everything is correct.

Save and export files.
```{r, include = FALSE}
currentDate <- Sys.Date()
csvFileName.summed <- paste("data_processed/Quantified_Summary_", Column.Type, "_", currentDate, ".csv", sep = "")
csvFileName.final <- paste("data_processed/Quantified_Measurements_", Column.Type, "_", currentDate, ".csv", sep = "")
csvFileName.perID <- paste("data_processed/Quantified_perSampID_", Column.Type, "_", currentDate, ".csv", sep = "")


write.csv(Final.Quantitative.Summed, csvFileName.summed, row.names = FALSE)
write.csv(Final.Quantitative, csvFileName.final, row.names = FALSE)
write.csv(Final.All.perSampID, csvFileName.perID, row.names = FALSE)
```

Clear environment.
```{r, include=FALSE}
rm(list = ls())
```